{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "#### General Dataset Info\n",
    " * Images are based on grey scale - max value for a pixel is 255.\n",
    " * Not provided with targets for the testing data, only for the training data.\n",
    "\n",
    "#### To do List\n",
    " * Normalize data - divide each pixel by 255 - ENSURE ALL ENTRIES ARE OF TYPE FLOAT TO ENSURE THERE IS NO DATA LOSS.\n",
    "   * Manually divide each pixel by 255.\n",
    " * Shuffle and split training data: 10% of 42000 images will be used to validate model to prevent overfitting.\n",
    "   * Preprocess data - normalizing and shuffling \n",
    " * Make CNN to analyze the images\n",
    "   * Use Linear rectified unit function for hidden layers.\n",
    "   * Use softmax function for output layer \n",
    "   * callback method will be based on if validation loss increases.\n",
    "   * The optimizer will be 'Adam'.\n",
    "   * The loss function will be a categorical cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the data for the CNN: extract and batch data from tensorflow datasets\n",
    "\n",
    "#Extract MNIST dataset\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "#Partition MNIST dataset\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#Scale pixels of images\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model - the model will have 2 hidden layers with a width of 70. Mess with hyperparameters later.\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units = 1600, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam',loss = \"sparse_categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "540/540 [==============================] - 31s 57ms/step - loss: 0.1535 - accuracy: 0.9526 - val_loss: 0.0731 - val_accuracy: 0.9775\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 28s 52ms/step - loss: 0.0464 - accuracy: 0.9859 - val_loss: 0.0459 - val_accuracy: 0.9868\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 28s 53ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0345 - val_accuracy: 0.9890\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 28s 53ms/step - loss: 0.0240 - accuracy: 0.9927 - val_loss: 0.0217 - val_accuracy: 0.9938\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 29s 53ms/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 0.0184 - val_accuracy: 0.9948\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 28s 52ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0160 - val_accuracy: 0.9950\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 28s 52ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0083 - val_accuracy: 0.9978\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 33s 61ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.0155 - val_accuracy: 0.9948\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 29s 55ms/step - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.0175 - val_accuracy: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x269199c96c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the data to the network. Hyper parameters can be messed with later.\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience = 2)\n",
    "\n",
    "\n",
    "model.fit(train_data,\n",
    "          epochs = max_epochs,\n",
    "          verbose = 1,\n",
    "          callbacks = [early_stop],\n",
    "          validation_data = validation_data\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 3s 3s/step - loss: 0.0379 - accuracy: 0.9899"
     ]
    }
   ],
   "source": [
    "#Making the predictions and recieving an array of probabilities for each image - 28000 predictions. Therefore 280000 prob.\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04. Test accuracy: 98.99%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
